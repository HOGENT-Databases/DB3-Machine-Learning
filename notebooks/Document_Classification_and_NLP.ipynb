{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Document Classification & NLP\n",
    "### Introduction\n",
    "In this section we look at a particular type of classification task, where the _objects are text documents_ such as  \n",
    "  \n",
    "- articles in newspapers\n",
    "- scientific papers\n",
    "- e-mails\n",
    "- reviews on websites\n",
    "- text messages  \n",
    "  \n",
    "The aim is to use a set of pre-classified documents to classify those that have not yet been seen. In principle we can use any of the standard methods of classification (Naïve Bayes, Logistic Regression, decision trees etc.) for this task, but datasets of text documents have a number of specific features compared with the datasets we have seen so far, which require some extra explanation.  \n",
    "  \n",
    "This subdiscipline of data mining is called **text mining**.   \n",
    "  \n",
    "Some of the problems we can solve using text mining are:  \n",
    "  \n",
    "- Is this text message SPAM or not? \n",
    "- To which of a set of predefined e-mail categories does this e-mail belong? \n",
    "- Is this paper about medicine?  Yes/No.  \n",
    "  \n",
    "### Natural Language Processing (NLP) \n",
    "Natural Language Processing (NLP) is a part of computer science and artificial intelligence which deals with _human languages_ (Text Mining in Python: Steps and Examples, 2019).\n",
    "In other words, NLP is a component of text mining that performs a special kind of linguistic analysis that essentially helps a machine “read” text. It uses a different methodology to _decipher the ambiguities in human language,_ including the following:  \n",
    "- automatic summarization\n",
    "- part-of-speech tagging\n",
    "- disambiguation\n",
    "- natural language understanding and recognition.\n",
    "\n",
    "**Part-speech-tagging**  is the process of determining to which word category a specific word belongs. Word categories can be for example _verbs_ , _nouns_ , _pronouns_ , etc. \n",
    "  \n",
    "Below we illustrate some of the preparatory task you usual have to perform in text mining projects. We make use of the Python library **NLTK** (**Natural Language Toolkit**, see (NLTK 3.4.5 documentation, sd)), which handles all kinds of NLP topics.  \n",
    "  \n",
    "#### Detect language\n",
    "First of all it’s clear that each language uses it’s own vocabulary. For that reason it might make sense to first determine the language for each document and create a separate classification model per language that occurs in the document set. Languages for which we have too few documents can be left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from langdetect import detect\n",
    "detect(\"Should I wear a mask when I'm exercising outside?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Lufthansa bietet wieder Urlaubsflüge an\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Coronavirus : une France coupée en deux, vers un déconfinement différencié par départements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nl'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Vanaf zondag mogen we vier mensen bij ons thuis ontvangen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "**Tokenization** is the process of breaking strings into tokens which in turn are small structures or units. This is typically used for breaking up a string that contains a complete sentence into separate words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Controversial', 'trials', 'in', 'which', 'volunteers', 'are', 'intentionally', 'infected', 'with', 'Covid-19', 'could', 'accelerate', 'vaccine', 'development', ',', 'according', 'to', 'the', 'World', 'Health', 'Organization', ',', 'which', 'has', 'released', 'new', 'guidance', 'on', 'how', 'the', 'approach', 'could', 'be', 'ethically', 'justified', 'despite', 'the', 'potential', 'dangers', 'for', 'participants', '.', 'So-called', 'challenge', 'trials', 'are', 'a', 'mainstream', 'approach', 'in', 'vaccine', 'development', 'and', 'have', 'been', 'used', 'in', 'malaria', ',', 'typhoid', 'and', 'flu', ',', 'but', 'there', 'are', 'treatments', 'available', 'for', 'these', 'diseases', 'if', 'a', 'volunteer', 'becomes', 'severely', 'sick', '.', 'For', 'Covid-19', ',', 'a', 'safe', 'dose', 'of', 'the', 'virus', 'has', 'not', 'been', 'established', 'and', 'there', 'are', 'no', 'failsafe', 'treatments', 'if', 'things', 'go', 'wrong', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jcor864\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# install the necessary files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# sample text for performing tokenization\n",
    "text = \"\"\"Controversial trials in which volunteers are intentionally infected with Covid-19 could accelerate vaccine\n",
    "development, according to the World Health Organization, which has released new guidance on how the approach could \n",
    "be ethically justified despite the potential dangers for participants.\n",
    "\n",
    "So-called challenge trials are a mainstream approach in vaccine development and have been used in malaria, \n",
    "typhoid and flu, but there are treatments available for these diseases if a volunteer becomes \n",
    "severely sick. For Covid-19, a safe dose of the virus has not been established and there are no failsafe \n",
    "treatments if things go wrong.\"\"\"\n",
    "\n",
    "# Passing the string text into word tokenize for splitting the text into tokens.\n",
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 5),\n",
       " ('are', 4),\n",
       " ('the', 4),\n",
       " ('in', 3),\n",
       " ('.', 3),\n",
       " ('a', 3),\n",
       " ('and', 3),\n",
       " ('trials', 2),\n",
       " ('which', 2),\n",
       " ('Covid-19', 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word removal\n",
    "Stop words (of, a, the, in, you, …) do occur very often but have little or no value in determining the topic of a document and are usually removed from texts. Obviously, stop words are language specific and can only be dropped after determining the language. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controversial trials volunteers intentionally infected covid-19 could accelerate vaccine development according world health organization released new guidance approach could ethically justified despite potential dangers participants so-called challenge trials mainstream approach vaccine development used malaria typhoid flu treatments available diseases volunteer becomes severely sick covid-19 safe dose virus established failsafe treatments things go wrong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcor864\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "    \n",
    "def remove_stopwords_en(text):\n",
    "    stop_words_en = set(stopwords.words('english')) \n",
    "    punctuations=\"?:!.,;<>/\\+-\"\n",
    "    # turn the string into a list of words based on separators (blank, comma, etc.)\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    # create a list of all words that are neither stopwords nor punctuations\n",
    "    result = [x for x in word_tokens if x not in stop_words_en and x not in punctuations]\n",
    "    \n",
    "    # create a new string of all remaining words\n",
    "    seperator = ' '\n",
    "    return seperator.join(result)\n",
    "\n",
    "print(remove_stopwords_en(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Words like welcome and welcoming are essentially about the same topic and can better be combined into a single term for better document classification. Another example is (playing, plays, played). This process is called stemming. However, this technique is far from accurate as it is based on certain rules (like removing suffixes -e or -ing) and can cause some irrelevant results. \n",
    "![](images/stemming.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome:welcom\n",
      "welcoming:welcom\n",
      "\n",
      "ball:ball\n",
      "balls:ball\n",
      "\n",
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n",
      "\n",
      "giving:give\n",
      "give:give\n",
      "given:given\n",
      "gave:gave\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stemming: examples\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "stm = [\"welcome\", \"welcoming\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + englishStemmer.stem(word))\n",
    "print()\n",
    "    \n",
    "stm = [\"ball\", \"balls\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + englishStemmer.stem(word))\n",
    "print()\n",
    "\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + englishStemmer.stem(word))\n",
    "print()\n",
    "\n",
    "stm = [\"giving\", \"give\", \"given\", \"gave\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + englishStemmer.stem(word))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worden:word\n",
      "wordt:wordt\n",
      "\n",
      "dader:dader\n",
      "daders:dader\n",
      "daad:dad\n",
      "\n",
      "las:las\n",
      "lezen:lez\n",
      "gelezen:gelez\n",
      "lees:les\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dutchStemmer=SnowballStemmer(\"dutch\")\n",
    "\n",
    "stm = [\"worden\", \"wordt\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + dutchStemmer.stem(word))\n",
    "print()\n",
    "    \n",
    "stm = [\"dader\", \"daders\", \"daad\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + dutchStemmer.stem(word))\n",
    "print()\n",
    "\n",
    "stm = [\"las\", \"lezen\", \"gelezen\", \"lees\"]\n",
    "for word in stm:\n",
    "    print(word + \":\" + dutchStemmer.stem(word))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controversi trial in which volunt are intent infect with covid-19 could acceler vaccin develop , accord to the world health organ , which has releas new guidanc on how the approach could be ethic justifi despit the potenti danger for particip . so-cal challeng trial are a mainstream approach in vaccin develop and have been use in malaria , typhoid and flu , but there are treatment avail for these diseas if a volunt becom sever sick . for covid-19 , a safe dose of the virus has not been establish and there are no failsaf treatment if thing go wrong .\n"
     ]
    }
   ],
   "source": [
    "# Stemming: replace words by stem\n",
    "def stemming_en(text):\n",
    "    word_tokens = word_tokenize(text.lower()) \n",
    "    seperator = ' '\n",
    "    result = [englishStemmer.stem(x) for x in word_tokens]\n",
    "    return seperator.join(result)\n",
    "\n",
    "print(stemming_en(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.  \n",
    "  \n",
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car.  \n",
    "  \n",
    "Both stemming and lemmatization are also language specific and are not yet available to the same extent for all languages in the Python libraries. (Jabeen, Hafsa; Datacamp, 2018)\n",
    "\n",
    "![](images/lemmatization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks: rock\n",
      "corpora: corpus\n",
      "gone:gone\n",
      "going:going\n",
      "went:went\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jcor864\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(\"rocks:\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora:\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "words = [\"gone\", \"going\", \"went\"]\n",
    "for word in words:\n",
    "    print(word + \":\" + lemmatizer.lemmatize(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controversial trial in which volunteer are intentionally infected with covid-19 could accelerate vaccine development , according to the world health organization , which ha released new guidance on how the approach could be ethically justified despite the potential danger for participant . so-called challenge trial are a mainstream approach in vaccine development and have been used in malaria , typhoid and flu , but there are treatment available for these disease if a volunteer becomes severely sick . for covid-19 , a safe dose of the virus ha not been established and there are no failsafe treatment if thing go wrong .\n"
     ]
    }
   ],
   "source": [
    "def lemmatizing_en(text):\n",
    "    word_tokens = word_tokenize(text.lower()) \n",
    "    seperator = ' '\n",
    "    result = [lemmatizer.lemmatize(x) for x in word_tokens]\n",
    "    return seperator.join(result)\n",
    "\n",
    "print(lemmatizing_en(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using n-grams\n",
    "Instead of counting single words as we did here, we could count sequences of words, like “clean match” and “close election”. Of course, to use this we will need some language aware tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing text documents for data mining\n",
    "#### Counting words\n",
    "In the introduction we saw we can use the classification methods we already know. So we have to find a way to represent a text document as a set of rows and columns. A simple method to represent documents is:\n",
    "- One row per document. Consider the term “document” to be any text one can access regardless of the format, from text in a word document to just a standalone string variable.\n",
    "- One column per unique word in the complete collection of documents\n",
    "- In each cell we then write the number of times (the frequency) the corresponding word occurs in the document. This is called the term frequency (tf) approach.  \n",
    "  \n",
    "Let’s look at a simple document: `“Intelligent applications create intelligent business processes”`.  \n",
    "The representation of this simple document is:\n",
    "\n",
    "<table>\n",
    "<tbody style=\"text-align:center\">\n",
    "<tr>\n",
    "<td width=\"88\">\n",
    "<p>intelligent</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>applications</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>create</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>business</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>processes</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td width=\"88\">\n",
    "<p>2</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>1</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>1</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>1</p>\n",
    "</td>\n",
    "<td width=\"88\">\n",
    "<p>1</p>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "You could determine the frequency distribution of the words in a token in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 5, 'are': 4, 'the': 4, 'in': 3, '.': 3, 'a': 3, 'and': 3, 'trials': 2, 'which': 2, 'Covid-19': 2, ...})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "\n",
    "text = \"\"\"Controversial trials in which volunteers are intentionally infected with Covid-19 could accelerate vaccine\n",
    "development, according to the World Health Organization, which has released new guidance on how the approach could \n",
    "be ethically justified despite the potential dangers for participants.\n",
    "\n",
    "So-called challenge trials are a mainstream approach in vaccine development and have been used in malaria, \n",
    "typhoid and flu, but there are treatments available for these diseases if a volunteer becomes \n",
    "severely sick. For Covid-19, a safe dose of the virus has not been established and there are no failsafe \n",
    "treatments if things go wrong.\"\"\"\n",
    "\n",
    "# Passing the string text into word tokenize for splitting the text into tokens.\n",
    "token = word_tokenize(text)\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the result will be a very sparse matrix with maybe thousands of columns as the number of columns equals the complete vocabulary size of the document set. Most attributes will be zero as a specific document may have just a few 100 (or even less) different words.  \n",
    "  \n",
    "However, term frequency is not the only possible approach from creating the feature values of your input.  \n",
    "  \n",
    "Another method is **term frequency – inverse document frequency (tf-idf)**, where each term is getting a score in each document that is calculated as:   \n",
    "   \n",
    "\\begin{equation}\n",
    "score\\ =\\ tf\\ \\ast\\ idf\n",
    "\\end{equation}  \n",
    "where\n",
    "\\begin{equation}\n",
    "tf=term\\ frequency\\ \\left(see\\ above\\right)\n",
    "\\end{equation} \n",
    "  \n",
    "\\begin{equation}\n",
    "idf_t=log\\left(\\frac{N}{df_t}\\right)\n",
    "\\end{equation}  \n",
    "  \n",
    "\\begin{equation} \n",
    "N=total\\ number\\ ofdocuments\n",
    "\\end{equation}  \n",
    "  \n",
    "\\begin{equation} \n",
    "df_t=the\\ number\\ of\\ documents\\ in\\ which\\ term\\ t\\ occurs\n",
    "\\end{equation}  \n",
    "  \n",
    "Clearly, this approach corrects for terms that occur in many documents. This does make sense since in document classification we really want to find the words in each document that distinguish that document from others in the collection. If a word occurs in (almost) all documents it isn’t of great help in categorizing this document.  \n",
    "  \n",
    "__Example__\n",
    "  \n",
    "_Document 1:_  \n",
    ">**36 dead, 47 injured in Shanghai New Year's celebration stampede**  \n",
    ">As Asia kicks off 2015, one New Year's celebration in Shanghai, China has resulted in 35 deaths and 42 injuries. China's CCTV America reports that the event, held in Chen Yi Square, near a waterfront area called the Bund, devolved into a stampede.  \n",
    "   \n",
    "_Document 2:_  \n",
    ">**Inside 'New Year's Rockin' Eve' live from Times Square**  \n",
    ">Welcome 2015 and welcome to New York City. Regardless of where you're welcoming the new year, all TVs will soon be tuned to  watch the ball drop on 'New Year's Rockin' Eve,'the annual televised event centered round the ceremonial ball drop in New York City's Times Square.  \n",
    "  \n",
    "_Document 3:_\n",
    ">**Gwyneth Paltrow Reveals She and Chris Martin Should Probably Have Stayed Married**  \n",
    ">In the latest issue of Harper's Bazaar U.K., actress and lifestyle guru Gwyneth Paltrow reflects on her relationship with ex-husband Chris Martin, and talks about the influence of her late father.\n",
    "\n",
    "  \n",
    "Now let’s compare _tf_ and _tf-idf_ for the three documents and for the words _new_ and _china_.  \n",
    "   \n",
    "$N = 3$  \n",
    "  \n",
    "In Docuument 1:  \n",
    "- new:  \n",
    "  \n",
    "  $tf = 2$  \n",
    "  $idf = log\\left(\\frac{3}{2}\\right)=0.18$  \n",
    "  $tf*idf = 0.36$  \n",
    "  \n",
    "  \n",
    "- china:  \n",
    "  \n",
    "  $tf = 2$  \n",
    "  $idf = log\\left(\\frac{3}{1}\\right)=0.48$  \n",
    "  $tf*idf = 0.96$  \n",
    "   \n",
    "We see that the words _china_ and _new_ both occur twice in document 1 but _china_ is getting a much higher _tf-idf_ score (0.96 vs. 0.36) because this is the only document in which it occurs. So the word _china_ is telling more about this document than the word _new_ and this is reflected in the _tf-idf_ score.  \n",
    "  \n",
    "You might also have noticed that we work with _welcom_ to count both welcome and welcoming in document 2, which means we have applied _stemming_ before calculating the scores. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}