{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Three of the most commonly used classification algorithms are:\n",
    "- Naïve Bayes Classficiation\n",
    "- Random Forest Classification\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Naïve Bayes Classification\n",
    "This algorithm uses the branch of Mathematics known as probability theory to find the most likely of the possible classifications.  \n",
    "  \n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.  \n",
    "To understand the naive Bayes classifier we need to understand the Bayes theorem. So let’s first discuss the Bayes Theorem.  \n",
    "  \n",
    "Bayes theorem is named after Rev. Thomas Bayes. It works on conditional probability. **Conditional probability is the probability that something will happen, given that something else has already occurred.** Using the conditional probability, we can calculate the probability of an event using its prior knowledge.  \n",
    "![](images/bayes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Below is the formula for calculating the conditional probability.  \n",
    "$P(H|E)=\\frac{P(E|H)*P(H)}{P(E)}$  \n",
    "where\n",
    "- P(H) is the probability of hypothesis H being true. This is known as the prior probability.\n",
    "- P(E) is the probability of the evidence (regardless of the hypothesis).\n",
    "- P(E|H) is the probability of the evidence given that hypothesis is true.\n",
    "- P(H|E) is the probability of the hypothesis given that the evidence is there.\n",
    "\n",
    "\n",
    "Naive Bayes is a kind of classifier which uses the Bayes Theorem. It predicts membership probabilities for each class such as the probability that a given record or data point belongs to a particular class.  The class with the highest probability is considered as the most likely class. This is also known as Maximum A Posteriori (MAP).  \n",
    "  \n",
    "Naive Bayes classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature.  \n",
    "  \n",
    "_Fruit Example_\n",
    "  \n",
    "Let's say that we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit:\n",
    "- Whether it is Long\n",
    "- Whether it is Sweet and\n",
    "- If its color is Yellow.\n",
    "  \n",
    "This is our 'training set', which exists of a table of 1000 lines (one per piece of fruit) and 3 yes/no input columns and 1 output column with the value banana, orange or “other fruit”.  The statistics of this table can be summarized as follows. \n",
    "\n",
    "![](images/fruit.png)\n",
    "  \n",
    "We will use this table to predict the type of any new fruit we encounter. We can pre-compute a lot of things about our fruit collection. The so-called \"Prior\" probabilities. (If we didn't know any of the fruit attributes, this would be our guess.) These are our base rates.\n",
    "  \n",
    "![](images/baserates.png)\n",
    "  \n",
    "Probability of \"Evidence\"\n",
    "  \n",
    "![](images/evidence.png)\n",
    "\n",
    "Probability of \"Likelihood\"\n",
    "  \n",
    "![](images/likelihood.png)\n",
    "  \n",
    "Given a Fruit, how to classify it?  \n",
    "    \n",
    "Let's say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?  \n",
    "  \n",
    "We can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and 'classify' our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set):\n",
    "  \n",
    "![](images/bayesformula.png)\n",
    "  \n",
    "By an overwhelming margin (0.252 >> 0.01875), we classify this Sweet/Long/Yellow fruit as likely to be a Banana. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Gaussian Naïve Bayes\n",
    "In the previous example it’s easy to calculate the probabilities because for each fruit we have three attributes (long, sweet, yellow) that can each have two values (yes/no) and we can count the number of fruit samples for each combination. However, in case your attributes can take lots of possible values (like shoe size) or are even continuous (like a person’s height or weight) it’s not longer that easy to calculate the probabilities for each combination in your dataset. In such a case you have to assume a probability distribution for your data samples, for instance assume that the height of a person is normally (or Gaussion) distributed.  \n",
    "  \n",
    "When configuring the Naïve Bayes classifier in your machine learning program you have to indicate the kind of distribution of your data.  \n",
    "  \n",
    "Obviously most Naïve Bayes implementation support the normal distribution, which is then called Gaussian Naive Bayes. In this classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution. Imagine that you have the following data, where the x and y values represent features and each dot has to be classified (there are two labels: blue and red):  \n",
    "  \n",
    "![](images/naivebayesbluered.png)\n",
    "\n",
    "One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. We can fit this model by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution. The result of this naïve Gaussian assumption is shown in the figure below: \n",
    "\n",
    "![](images/naivebayesblueredgaussian.png)\n",
    "\n",
    "Each cloud of dots has to be interpreted as a three dimensional gauss curve, where the color saturation represents the z value or height: the darker the higher. A three-dimensional gauss curve is illustrated below.  \n",
    "  \n",
    "![](images/gaussian.png)\n",
    "  \n",
    "  \n",
    "#### Multinomial Naïve Bayes\n",
    "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label. Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution. The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates from a simple multinomial distribution. We will use Multinomial Naïve Bayes in the chapter about Natural Language Processing (NLP) for instance when predicting spam based on count rates of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
