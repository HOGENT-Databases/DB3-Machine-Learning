{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model optimization by parameter hypertuning\n",
    "\n",
    "In the previous example we used as model hyperparameters: \n",
    "\n",
    "* 1 hidden layer with 32 neurons \n",
    "* 1 hidden layer with 16 neurons \n",
    "* learning rate = 0.001 \n",
    "* batch size = 16 \n",
    "* nr of epochs = 20 \n",
    "\n",
    "and obtained:\n",
    "![](images/model_optimization.png)\n",
    "\n",
    "#### Conclusion\n",
    "With this initial model, we can see that the learning curves have not converged yet: the accuracy is still increasing and the loss decreasing. The initial number of epochs set at 20 was not long enough. Plus, the curves are very bumpy. To fix both these problems, we can increase the number of epochs to reach convergence, and increase the batch size to make the curves smoother.\n",
    "\n",
    "### Exercise 1: increase number of epochs and batch size\n",
    "\n",
    "Use new model parameters: \n",
    "\n",
    "* 1 hidden layer with 32 neurons\n",
    "* 1 hidden layer with 16 neurons\n",
    "* learning rate = 0.001\n",
    "* batch size = 512\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We can see that the shapes of the curves now look better. However, training accuracy perhaps can be improved with a more complex architecture. \n",
    "  \n",
    "Our goal for now is to make the architecture powerful enough, so we will only look at the training accuracy and loss, and improve them the best as possible. We will explore both layer width and network depth. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 2: improve the architecture to make it powerful enough\n",
    "\n",
    "\n",
    "The initial model had 2 hidden layers with 32 and 16 neurons each. We will first try to increase the number of neurons to 128 for the first layer and 64 for the second layer, and see how the model perfoms with a wider architecture.\n",
    "Model:\n",
    "* 1 hidden layer with 128 neurons\n",
    "* 1 hidden layer with 64 neurons\n",
    "* learning rate = 0.001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We can see that the training accuracy has significantly increased to be extremely close to 1. The loss has also decreased to be very close to 0. Adding more neurons to the hidden layers has increased the number of trainable parameters. This explains the better performance. Even if we see that the model is overfitting, our goal is to increase performance on the training set.\n",
    "We will try to increase the number of neurons to 512 and 256. We will also decrease learning rate to 0.0001.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 3: increase number of neurons and decrease learning rate\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 512 neurons\n",
    "* 1 hidden layer with 256 neurons\n",
    "* learning rate = 0.0001\n",
    "* batch size = 512\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "As expected, training accuracy has well increased with double the number of neurons per hidden layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 4: make network deeper\n",
    "\n",
    "We will now try to make our network deeper. We will add a third hidden layer in order to try to improve our neural network even more.\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 512 neurons\n",
    "* 1 hidden layer with 256 neurons\n",
    "* 1 hidden layer with 128 neurons\n",
    "* learning rate = 0.0001\n",
    "* batch size = 512\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We will try to make our architecture even more complex by increasing the number of neurons."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 5: increase number of neurons\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 800 neurons\n",
    "* 1 hidden layer with 400 neurons\n",
    "* 1 hidden layer with 200 neurons\n",
    "* learning rate = 0.0001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "With 3 hidden layers and even more neurons, we obtain a very powerful model that reaches perfect accuracy again on the training data. Learning convergence is also verified.\n",
    "To speed up processing time, we can try to increase the learning rate back from 0.0001 to 0.001."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 6: increase learning rate\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 800 neurons\n",
    "* 1 hidden layer with 400 neurons\n",
    "* 1 hidden layer with 200 neurons\n",
    "* learning rate = 0.001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "As expected, processing time is faster. The performance offered by this model is satisfying : very high accuracy and very low error on the training data.\n",
    "We would like to try to deepen our architecture once again and add a fourth hidden layer to see how the performance would evolve.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 7: further deepen architecture\n",
    "Model:\n",
    "* 1 hidden layer with 800 neurons\n",
    "* 1 hidden layer with 400 neurons\n",
    "* 1 hidden layer with 400 neurons\n",
    "* 1 hidden layer with 200 neurons\n",
    "* learning rate = 0.001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The curves are very bumpy. We choose to keep the simpler model with 3 hidden layers.\n",
    "Now that we have obtained an architecture that offers satisfying results on the training data, the previous model (3 hidden layers with 800, 400 and 200 neurons) will be kept. However we can notice a generalization gap between the training and validation accuracy curves. We will then add dropout as to prevent the model from overfitting.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 8: add dropout \n",
    "\n",
    "First, we will add dropout to the hidden layers only. Dropout should help the model to generalize better on unseen data. Indeed, a certain fraction of neurons are switched off. As each node has a certain probability of being dropped out, each neuron is forced to be important on its own and rely less on the other neurons. That is the reason why dropout enables a reduced probability of overfitting.\n",
    "\n",
    "We first choose a dropout probability of 0,1 which means that each node has a probability of 0,1 of being dropped out.\n",
    "\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 800 neurons\n",
    "* Dropout = 0.1\n",
    "* 1 hidden layer with 400 neurons\n",
    "* Dropout = 0.1\n",
    "* 1 hidden layer with 200 neurons\n",
    "* Dropout = 0.1\n",
    "* learning rate = 0.0001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "With a dropout probability of 0,1, the validation loss is lower. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}