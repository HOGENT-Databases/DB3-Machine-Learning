{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model optimization by parameter hypertuning\n",
    "\n",
    "In the previous example we used as model hyperparameters: \n",
    "\n",
    "* 1 hidden layer with 32 neurons \n",
    "* 1 hidden layer with 16 neurons \n",
    "* learning rate = 0.001 \n",
    "* batch size = 16 \n",
    "* nr of epochs = 20 \n",
    "\n",
    "and obtained:\n",
    "![](images/model_optimization.png)\n",
    "\n",
    "#### Conclusion\n",
    "With this initial model, we can see that the learning curves have not converged yet: the accuracy is still increasing and the loss decreasing. The initial number of epochs set at 20 was not long enough. Plus, the curves are very bumpy. To fix both these problems, we can increase the number of epochs to reach convergence, and increase the batch size to make the curves smoother.\n",
    "\n",
    "### Exercise 1: increase number of epochs and batch size\n",
    "\n",
    "Use new model parameters: \n",
    "\n",
    "* 1 hidden layer with 32 neurons\n",
    "* 1 hidden layer with 16 neurons\n",
    "* learning rate = 0.001\n",
    "* batch size = 512\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We can see that the shapes of the curves now look better. However, training accuracy perhaps can be improved with a more complex architecture. \n",
    "  \n",
    "Our goal for now is to make the architecture powerful enough, so we will only look at the training accuracy and loss, and improve them the best as possible. We will explore both layer width and network depth. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 2: improve the architecture to make it powerful enough\n",
    "\n",
    "\n",
    "The initial model had 2 hidden layers with 32 and 16 neurons each. We will first try to increase the number of neurons to 128 for the first layer and 64 for the second layer, and see how the model perfoms with a wider architecture.\n",
    "Model:\n",
    "* 1 hidden layer with 128 neurons\n",
    "* 1 hidden layer with 64 neurons\n",
    "* learning rate = 0.001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We can see that the training accuracy has significantly increased to be extremely close to 1. The loss has also decreased to be very close to 0. Adding more neurons to the hidden layers has increased the number of trainable parameters. This explains the better performance. Even if we see that the model is overfitting, our goal is to increase performance on the training set.\n",
    "We will try to increase the number of neurons to 512 and 256. We will also decrease learning rate to 0.0001.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 3: increase number of neurons and decrease learning rate\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 512 neurons\n",
    "* 1 hidden layer with 256 neurons\n",
    "* learning rate = 0.0001\n",
    "* batch size = 512\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "As expected, training accuracy has well increased with double the number of neurons per hidden layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 4: make network deeper\n",
    "\n",
    "We will now try to make our network deeper. We will add a third hidden layer in order to try to improve our neural network even more.\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 512 neurons\n",
    "* 1 hidden layer with 256 neurons\n",
    "* 1 hidden layer with 128 neurons\n",
    "* learning rate = 0.0001\n",
    "* batch size = 512\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We will try to make our architecture even more complex by increasing the number of neurons."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 5: increase number of neurons\n",
    "\n",
    "Model:\n",
    "* 1 hidden layer with 800 neurons\n",
    "* 1 hidden layer with 400 neurons\n",
    "* 1 hidden layer with 200 neurons\n",
    "* learning rate = 0.0001\n",
    "* batch size = 1024\n",
    "* nr of epochs = 50\n",
    "\n",
    "Make the new plot: \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "With 3 hidden layers and even more neurons, we obtain a very powerful model that reaches perfect accuracy again on the training data. Learning convergence is also verified.\n",
    "To speed up processing time, we can try to increase the learning rate back from 0.0001 to 0.001."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}