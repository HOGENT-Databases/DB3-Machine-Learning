{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "colab": {
      "name": "Using pretrained word embeddings Blog Gender.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAMKfJvV85iq"
      },
      "source": [
        "\n",
        "## Pre-trained word embeddings: case Blog Gender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpMifYkx85is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7a89ae-8c7f-41b7-d466-cbc95c4b9e78"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "import pandas as pd\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 2020\n",
        "np.random.seed(seed)  \n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Embedding, Conv1D,  MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "import nltk\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms0fWnI2PyL2"
      },
      "source": [
        "# helper functions for visualisation\r\n",
        "# plotting the loss functions used in this notebook\r\n",
        "# we plot the loss we want to optimise on the left (in this case: accuracy)\r\n",
        "def plot_history(history):\r\n",
        "  plt.figure(figsize = (12,4))\r\n",
        "  plt.subplot(1,2,1)\r\n",
        "\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.ylabel('Accuracy')\r\n",
        "  plt.plot(history.epoch, np.array(history.history['accuracy']),'g-',\r\n",
        "           label='Train accuracy')\r\n",
        "  plt.plot(history.epoch, np.array(history.history['val_accuracy']),'r-',\r\n",
        "           label = 'Validation accuracy')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  plt.subplot(1,2,2)\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.ylabel('Loss minimised by model')\r\n",
        "  plt.plot(history.epoch, np.array(history.history['loss']),'g-',\r\n",
        "           label='Train loss')\r\n",
        "  plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\r\n",
        "           label = 'Validation loss')\r\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NC8R1hQ_Ipn",
        "outputId": "2e6f0c76-478f-4195-fc92-1a5d051ebef1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-FrOQlg_Z-D",
        "outputId": "ea31303e-3fe5-4d0b-ad5d-5c1ac16a66df"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/HOGENT-Databases/DB3-Workshops/master/data/blog-gender-dataset.csv'\r\n",
        "df_dataset = pd.read_csv(url)\r\n",
        "df_dataset.columns\r\n",
        "df_dataset.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "j2f7FcYqjD6k",
        "outputId": "2abbaa31-019a-4299-976a-2a80ac6b1e24"
      },
      "source": [
        "df_dataset[\"gender\"] = df_dataset[\"gender\"].str.strip()\r\n",
        "df_dataset[\"gender\"] = df_dataset[\"gender\"].str.upper()\r\n",
        "\r\n",
        "df_dataset.groupby(\"gender\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj27HLdGm30E"
      },
      "source": [
        "df_dataset.fillna(value='', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHUQqsAFzL5W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "22d7917c-ba05-4dc4-bd73-7897e08d7af6"
      },
      "source": [
        "# Calculate the number of words\r\n",
        "df_dataset['numberOfWords'] = df_dataset.text.str.split().apply(len)\r\n",
        "df_dataset.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "EaccmB11Q_7k",
        "outputId": "d98868a2-ff6b-4a08-de1c-f8360051a1be"
      },
      "source": [
        "df_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "J8Jl5bZZRIbF",
        "outputId": "16bcdcf6-1cec-41a4-98ff-5610f98369b9"
      },
      "source": [
        "# Changing spam and ham into 0 and 1\r\n",
        "df_dataset['gender'] = np.where(df_dataset['gender'] == \"M\", 0, 1)\r\n",
        "df_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5zHfTOBQs7",
        "outputId": "807cdfdb-8d61-480d-9e23-54fd722511dd"
      },
      "source": [
        "# Extract a training & validation split\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X = df_dataset.drop(['gender','numberOfWords'],axis=1)\r\n",
        "y = df_dataset['gender']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30)\r\n",
        "\r\n",
        "print('X_train shape:', X_train.shape)\r\n",
        "print('X_test shape:', X_test.shape)\r\n",
        "print(X_train.shape[0], 'train samples')\r\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsAXPQD9cViO",
        "outputId": "5651727b-4258-476e-f16f-8247df13b0f9"
      },
      "source": [
        "X_train = np.asarray(X_train)\r\n",
        "X_test = np.asarray(X_test)\r\n",
        "\r\n",
        "print('X_train shape:', X_train.shape)\r\n",
        "print(type(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YNIcfYeQ8QB",
        "outputId": "5cae39ce-7ee1-4e43-a9cb-44a21a0917c9"
      },
      "source": [
        "# the labels from the downloaded data are integer numbers\r\n",
        "# for a multi-class classification task, we again convert each integer\r\n",
        "# to a vector with 19 zeros and a single '1', corresponding to the right class\r\n",
        "num_classes = 2\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
        "\r\n",
        "\r\n",
        "# look at the new labels for the first sample\r\n",
        "print(y_test[0])\r\n",
        "print('y_train shape:', y_train.shape)\r\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSvPJXT0C0Q8"
      },
      "source": [
        "### Create a vocabulary index\r\n",
        "\r\n",
        "Let's use the TextVectorization to index the vocabulary found in the dataset. Later, we'll use the same layer instance to vectorize the samples.\r\n",
        "\r\n",
        "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to be actually 40 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9vR-3X7Cy7D"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "\r\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=500)\r\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\r\n",
        "vectorizer.adapt(text_ds)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQx8k5FECtSU",
        "outputId": "cbf5f276-9709-417b-a323-bfaf6e6d077d"
      },
      "source": [
        "# You can retrieve the computed vocabulary used via vectorizer.get_vocabulary(). \r\n",
        "# Let's print the top 5 words:\r\n",
        "vectorizer.get_vocabulary()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrXr5Ry8Dn_j",
        "outputId": "08b0fb70-2c0a-42e8-8b69-c5026d7b0ba0"
      },
      "source": [
        "# Let's vectorize a test sentence:\r\n",
        "output = vectorizer([[\"i saw the cat sat on the mat\"]])\r\n",
        "output.numpy()[0, :8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Iuzq59D-fb"
      },
      "source": [
        "As you can see, \"i\" gets represented as \"2\". Why not 0, given that \"i\" was the first word in the vocabulary? That's because index 0 is reserved for padding and index 1 is reserved for \"out of vocabulary\" tokens.\r\n",
        "\r\n",
        "Here's a dict mapping words to their indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYtTe3gQDwEA"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\r\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Eax6cpESR2"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHO0wHTTENcE",
        "outputId": "d0b538f3-0b83-43e1-9062-43e6d54ae8fb"
      },
      "source": [
        "test = [\"i\",\"saw\",\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\r\n",
        "[word_index[w] for w in test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymT35BgXEbM7"
      },
      "source": [
        "### Load pre-trained word embeddings\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is4_imN2E2Z3"
      },
      "source": [
        "The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\r\n",
        "\r\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcp1cFCvFASx",
        "outputId": "65d6ec5e-3d60-414c-8fa7-ecded5a2f595"
      },
      "source": [
        "path_to_glove_file = '/content/gdrive/My Drive/glove.6B.100d.txt'\r\n",
        "\r\n",
        "embeddings_index = {}\r\n",
        "with open(path_to_glove_file) as f:\r\n",
        "    for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "      embeddings_index[word] = coefs\r\n",
        "\r\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-R9zKnFSIKF",
        "outputId": "dd93adb9-cc84-4f52-bf4f-514a92544a05"
      },
      "source": [
        "print(embeddings_index['cat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fo3bId0FUVh"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrIogMZQFXw9",
        "outputId": "4a8b04ab-c25f-4867-e9a7-27bf4ece41ca"
      },
      "source": [
        "num_tokens = len(voc) + 2\r\n",
        "# each word is represented by a vector of 100 floats (glove.6B.100d.txt)\r\n",
        "embedding_dim = 100\r\n",
        "hits = 0\r\n",
        "misses = 0\r\n",
        "missed_words = []\r\n",
        "\r\n",
        "# Prepare embedding matrix\r\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n",
        "# word_index is a dictionary that maps each word to an index\r\n",
        "# we loop through all the words of word_index.items()\r\n",
        "for word, i in word_index.items():\r\n",
        "# we try to retrieve the vector of 100 floats for this word out of embeddings_index  \r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "# if we found the corresponding vector of 100 floats    \r\n",
        "    if embedding_vector is not None:\r\n",
        "      # we put the vector on position i of embedding_matrix\r\n",
        "        embedding_matrix[i] = embedding_vector\r\n",
        "        hits += 1    \r\n",
        "    else:\r\n",
        "      # Words not found in embedding index will be all-zeros.    \r\n",
        "        misses += 1\r\n",
        "        missed_words.append(word)\r\n",
        "        \r\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\r\n",
        "\r\n",
        "print(\"*** Missed words = words not in word_index ***\")\r\n",
        "print(missed_words[0:10])\r\n",
        "print()\r\n",
        "print(\"*** i has which index in word_index? ***\")\r\n",
        "index_i = word_index['i']\r\n",
        "print(index_i)\r\n",
        "print()\r\n",
        "print(\"*** the vector of 100 floats representing i ***\")\r\n",
        "print(embedding_matrix[index_i])\r\n",
        "print()\r\n",
        "print(\"*** cat has which index in word_index? ***\")\r\n",
        "index_cat = word_index['cat']\r\n",
        "print(index_cat)\r\n",
        "print()\r\n",
        "print(\"*** the vector of 100 floats representing cat ***\")\r\n",
        "print(embedding_matrix[index_cat])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P-VZMzkIdqT"
      },
      "source": [
        "### Build the model\r\n",
        "\r\n",
        "A simple 1D convnet with global max pooling and a classifier at the end.\r\n",
        "We load the pre-trained word embeddings matrix into an Embedding layer.\r\n",
        "\r\n",
        "Note that we set trainable=False so as to keep the embeddings fixed (we don't want to update them during training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez33ioRO7_3k"
      },
      "source": [
        "num_classes = 2\r\n",
        "\r\n",
        "def initial_model():\r\n",
        "    # we create a variable called model, and we set it equal to an instance of a Sequential object.\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Embedding(num_tokens, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False))\r\n",
        "    model.add(Conv1D(64,activation='relu',kernel_size=3))\r\n",
        "    model.add(MaxPooling1D(3))  \r\n",
        "    model.add(Conv1D(64,activation='relu',kernel_size=3))\r\n",
        "    model.add(GlobalMaxPooling1D())\r\n",
        "    model.add(Dense(16, activation='relu', kernel_initializer='he_uniform'))    \r\n",
        "    model.add(Dropout(0.5))      \r\n",
        "    model.add(Dense(num_classes, activation='softmax'))\r\n",
        "\r\n",
        "\r\n",
        "    # Before we can train our model, we must compile it\r\n",
        "    # To the compile() function, we are passing the optimizer, the loss function, and the metrics that we would like to see. \r\n",
        "    # Notice that the optimizer we have specified is called Adam. Adam is just a variant of SGD. \r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                  optimizer= tf.keras.optimizers.Adam(learning_rate = 0.001),\r\n",
        "                  metrics=['accuracy']) \r\n",
        "    return model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aElU90qXVV0I"
      },
      "source": [
        "### Train the model\r\n",
        "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays are right-padded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x07Drl07zwE",
        "outputId": "e55edba5-6524-4e49-a61b-db72e4ec9baf"
      },
      "source": [
        "X_train_final = vectorizer(np.array([s for s in X_train])).numpy()\r\n",
        "X_test_final = vectorizer(np.array([s for s in X_test])).numpy()\r\n",
        "\r\n",
        "y_train_final = np.array(y_train)\r\n",
        "y_test_final = np.array(y_test)\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)\r\n",
        "print(X_train_final.shape)\r\n",
        "print(X_test_final.shape)\r\n",
        "print(y_train_final.shape)\r\n",
        "print(y_test_final.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpGrdwWQ85iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4b758e-0cb3-4396-edb9-93bec0994474"
      },
      "source": [
        "model_1 = initial_model()\r\n",
        "model_1.summary()\r\n",
        "\r\n",
        "\r\n",
        "# We now add batch size to the mix of training parameters\r\n",
        "# If you don't specify batch size below, all training data will be used for each learning step\r\n",
        "batch_size = 32\r\n",
        "epochs = 10\r\n",
        "\r\n",
        "history_1 = model_1.fit(X_train_final, y_train_final,\r\n",
        "                    batch_size=batch_size,\r\n",
        "                    epochs=epochs,\r\n",
        "                    verbose=1,\r\n",
        "                    validation_data=(X_test_final, y_test_final)\r\n",
        "                    )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "nCkT4JxmfP73",
        "outputId": "2daa62e1-ecd2-4acb-e215-80c64378af2f"
      },
      "source": [
        "# model_1 now contains the model at the end of the training run\r\n",
        "# We analyse the result:\r\n",
        "\r\n",
        "[train_loss, train_accuracy] = model_1.evaluate(X_train_final, y_train_final, verbose=0)\r\n",
        "print(\"Training set Accuracy:{:7.2f}\".format(train_accuracy))\r\n",
        "print(\"Training set Loss:{:7.4f}\\n\".format(train_loss))\r\n",
        "\r\n",
        "[val_loss, val_accuracy] = model_1.evaluate(X_test_final, y_test_final, verbose=0)\r\n",
        "print(\"Validation set Accuracy:{:7.2f}\".format(val_accuracy))\r\n",
        "print(\"Validation set Loss:{:7.4f}\\n\".format(val_loss))\r\n",
        "\r\n",
        "#Now we visualise what happened during training\r\n",
        "plot_history(history_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fACX0ULsgDjG",
        "outputId": "0fa3a269-de45-41dc-9919-c3ff2d740d0e"
      },
      "source": [
        "X_example = vectorizer(np.array([s for s in [\"My new dress is awesome\"]])).numpy()\r\n",
        "pred = model_1.predict([X_example])\r\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbMA5Svgg6od",
        "outputId": "d9c7f003-a977-46c8-bb9c-a9cbfa7f34ec"
      },
      "source": [
        "X_example = vectorizer(np.array([s for s in [\"Last night I was playing the FIFA soccer game\"]])).numpy()\r\n",
        "pred = model_1.predict([X_example])\r\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}