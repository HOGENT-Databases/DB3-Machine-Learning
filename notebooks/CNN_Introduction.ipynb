{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Introduction CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpF7oLPizf3f"
      },
      "source": [
        "## Convolutional Neural Networks   \n",
        "When classifying images with (simple) feed forward neural networks we noticed that, due to the __linearizing__, we ignore typical image features like shapes or edges. To cope with this more advanced neural networks are often used in computer vision applications. In this section we discuss convolutional neural networks, also called convnets or CNNs. They’re also used in non-vision applications, such as NLP, recommender systems and much more. \n",
        "  \n",
        "The concept of convolution is based on the observation that images are rather stationary, which means they have a lot of repeating patches.  \n",
        "![](https://github.com/HOGENT-Databases/DB3-Workshops/blob/master/notebooks/images/cnnbird.png?raw=1)\n",
        "\n",
        "So instead of focusing on the entire image, which would be very compute intensive, we can better focus on patch levels. This is accomplished by convolving (Nederlands = rollen) a patch over the image resulting in a filtered image with a reduced dimension. That patch (or kernel) is a small area (typical a 3 x 3 square) that shifts over the original image from left to right and from top to bottom and that performs a simple arithmetic operation with the corresponding square in the image.  \n",
        "  \n",
        "Let’s examine convolution on a 6x6 image (Deitel & Deitel, 2019). Consider the following diagram in which the 3x3 shaded square represents the kernel – the numbers are simply position numbers showing the order in which the kernels are visited and processed. \n",
        "![](https://github.com/HOGENT-Databases/DB3-Workshops/blob/master/notebooks/images/cnnlayer.png?raw=1) \n",
        "You can think of the kernel as a __sliding window__ that the convolution layer moves one pixel at a time left-to-right across the image. When the kernel reaches the right edge, the convolution layer moves the kernel one pixel down and repeats the left-to-right process.  \n",
        "  \n",
        "The convolution layer performs mathematical calculations using those nine features to “learn” about them, then outputs __one new feature__ to position 1 in the layer’s output. An example of such a kernel and the calculations is shown below. \n",
        "![](https://github.com/HOGENT-Databases/DB3-Workshops/blob/master/notebooks/images/cnnkernel.png?raw=1)\n",
        "In the above example the value in the upper left corner of the output feature map is calculated as:  \n",
        "\\begin{equation}  \n",
        "(0 * 0) + (0 * 0) + (1 * 1) + (0 *1) + (1 * 0) + (0 * 1) + (1 * 1) + (0 * 1) + (1 * 1) = 3\n",
        "\\end{equation}\n",
        "  \n",
        "The kernel values are chosen by Keras based on best-practices.  \n",
        "  \n",
        "By looking at features near one another, the network begins to recognize features like edges, straight lines and curves. This might look like magic but it turns out that applying several convolutional layers results in learning essential features of the original image like in the examples below. \n",
        "![](https://github.com/HOGENT-Databases/DB3-Workshops/blob/master/notebooks/images/cnnresults.png?raw=1)   \n",
        "  \n",
        "Next, the convolution layer moves the kernel one pixel to the right (known as the stride) to position 2 in the output layer. This new position overlaps with two of the three columns in the previous position, so that the convolution layer can learn from all the features that touch one another. The layer learns from the nine features in the kernel in position 2 and outputs one new feature in position 2 of the output, as in:\n",
        "![](https://github.com/HOGENT-Databases/DB3-Workshops/blob/master/notebooks/images/cnnlayer2.png?raw=1)   \n",
        "The complete pass of the image left-to-right and top-to-bottom is called a __filter__. For a 3x3 kernel, the filter dimensions (4x4 in our sample above), will be two less than the input dimensions (6x6). In the case of the MNIST digits for each 28x28 MNIST image, the filter will be 26x26.  \n",
        "  \n",
        "The number of filters in the convolutional layer is commonly 32 or 64 when processing small images like those in MNIST, and each filter produces different results, since each kernel has 0’s and 1’s in different positions. The number of filters depends on the image dimensions – higher-resolution images have more features, so they require more filters. The set of filters produced by a convolution layer is called a __feature map__.  \n",
        "  \n",
        "Subsequent convolution layers combine features from previous feature maps to recognize larger features and so on. If we were doing facial recognition, early layers might recognize lines, edges, and curves, and subsequent layers might begin combining those into larger features like eyes, eyebrows, noses, ears and mouths. Once a network learns a feature because of convolution, it can recognize that feature anywhere in the image. This is one of the reasons that convnets are used for object recognition in images.  \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_fkBUKe12rQ"
      },
      "source": [
        "### Input And Output Channels\n",
        "\n",
        "Suppose that this grayscale image (single color channel) of a seven from the MNIST data set is our input:\n",
        "\n",
        "![](images/mnist_7.png)\n",
        "\n",
        "Let’s suppose that we have four 3 x 3 filters for our first convolutional layer, and these filters are filled with the values you see below. These values can be represented visually by having -1s correspond to black, 1s correspond to white, and 0s correspond to grey.\n",
        "\n",
        "![](images/convolutional_layer_with_4_filters.PNG)\n",
        "\n",
        "If we convolve our original image of a seven with each of these four filters individually, this is what the output would look like for each filter:\n",
        "\n",
        "![](images/output_channels_of_the_convolutional_layer.PNG)\n",
        "\n",
        "We can see that all four of these filters are detecting edges. In the output channels, the brightest pixels can be interpreted as what the filter has detected. In the first one, we can see detects top horizontal edges of the seven, and that’s indicated by the brightest pixels (white).\n",
        "\n",
        "The second detects left vertical edges, again being displayed with the brightest pixels. The third detects bottom horizontal edges, and the fourth detects right vertical edges.\n",
        "\n",
        "These filters, as we mentioned before, are really basic and just detect edges. These are filters we may see towards the start of a convolutional neural network. More complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the ones shown here:\n",
        "\n",
        "![](images/CNN_layer_2_filters.PNG)\n",
        "\n",
        "We can see the shapes that the filters on the left detected from the images on the right. We can see circles, curves and corners. As we go further into our layers, the filters are able to detect much more complex patterns like dog faces or bird legs shown here:\n",
        "\n",
        "![](images/CNN_layer_4_filters.PNG)\n",
        "\n",
        "The amazing thing is that the pattern detectors are derived automatically by the network. The filter values start out with random values, and the values change as the network learns during training. The pattern detecting capability of the filters emerges automatically.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRHIhkhm12ws"
      },
      "source": [
        "### Zero Padding In Convolutional Neural Networks\n",
        "\n",
        "**Convolutions Reduce Channel Dimensions**\n",
        "\n",
        "\n",
        "Each convolutional layer has some number of filters that we define, and each of these filters has a dimension. \n",
        "\n",
        "When a filter convolves a given input channel, it gives us an output channel. This output channel is a matrix of pixels with the values that were computed during the convolutions that occurred on the input channel.\n",
        "\n",
        "When this happens, the dimensions of our image are reduced.\n",
        "\n",
        "For ease of visualizing this, let’s look at a small scale example. Here we have an input of size 4 x 4 and then a 3 x 3 filter. Let’s look at how many times we can convolve our input with this filter, and what the resulting output size will be.\n",
        "\n",
        "![](images/zero_padding_example.png)\n",
        "\n",
        "This means that when this 3 x 3 filter finishes convolving this 4 x 4 input, it will give us an output of size 2 x 2.\n",
        "\n",
        "We see that the resulting output is 2 x 2, while our input was 4 x 4, and so again, just like in our larger example with the image of a seven, we see that our output is indeed smaller than our input in terms of dimensions.\n",
        "\n",
        "We can know ahead of time by how much our dimensions are going to shrink. In general, if our image is of size n x n, and we convolve it with an f x f filter, then the size of the resulting output is (n - f + 1) x (n - f + 1).\n",
        "\n",
        "Let’s see if this holds up with our example here.\n",
        "\n",
        "Our input was size 4 x 4, so 4 would be our n, and our filter was 3 x 3, so 3 would be our f. Substituting these values in our formula, we have:\n",
        "\n",
        "(n - f + 1) = (4 - 3) + 1 = 2\n",
        "\n",
        "Indeed, this gives us a 2 x 2 output channel, which is exactly what we saw a moment ago. This holds up for the example with the larger input of the seven as well, so check that for yourself to confirm that the formula does indeed give us the same result of an output of size 26 x 26 that we saw when we visually inspected it.\n",
        "\n",
        "**Issues With Reducing The Dimensions**\n",
        "\n",
        "\n",
        "Consider the resulting output of the image of the MNIST dataset again (28 x 28). It doesn’t really appear to be a big deal that this output is a little smaller than the input, right?\n",
        "\n",
        "We didn’t lose that much data or anything because most of the important pieces of this input are kind of situated in the middle. But we can imagine that this would be a bigger deal if we did have meaningful data around the edges of the image.\n",
        "\n",
        "Additionally, we only convolved this image with one filter. What happens as this original input passes through the network and gets convolved by more filters as it moves deeper and deeper?\n",
        "\n",
        "Well, what’s going to happen is that the resulting output is going to continue to become smaller and smaller. This is a problem.\n",
        "\n",
        "If we start out with a 4 x 4 image, for example, then just after a convolutional layer or two, the resulting output may become almost meaningless with how small it becomes. Another issue is that we’re losing valuable data by completely throwing away the information around the edges of the input.\n",
        "\n",
        "What can we do here?\n",
        "\n",
        "**Zero Padding To The Rescue**\n",
        "\n",
        "\n",
        "Zero padding is a technique that allows us to preserve the original input size. This is something that we specify on a per-convolutional layer basis. With each convolutional layer, just as we define how many filters to have and the size of the filters, we can also specify whether or not to use padding.\n",
        "\n",
        "What Is Zero Padding?\n",
        "We now know what issues zero padding combats against, but what actually is it?\n",
        "\n",
        "Zero padding occurs when we add a border of pixels all with value zero around the edges of the input images. This adds kind of a padding of zeros around the outside of the image, hence the name zero padding. Going back to our small example from earlier, if we pad our input with a border of zero valued pixels, let’s see what the resulting output size will be after convolving our input.\n",
        "\n",
        "![](images/zero_padding_example_2.png)\n",
        "\n",
        "We see that our output size is indeed 4 x 4, maintaining the original input size. Now, sometimes we may need to add more than a border that’s only a single pixel thick. Sometimes we may need to add something like a double border or triple border of zeros to maintain the original size of the input. This is just going to depend on the size of the input and the size of the filters.\n",
        "\n",
        "The good thing is that most neural network APIs figure the size of the border out for us. All we have to do is just specify whether or not we actually want to use padding in our convolutional layers.\n",
        "\n",
        "Valid And Same Padding\n",
        "There are two categories of padding. One is referred to by the name valid. This just means no padding. If we specify valid padding, that means our convolutional layer is not going to pad at all, and our input size won’t be maintained.\n",
        "\n",
        "The other type of padding is called same. This means that we want to pad the original input before we convolve it so that the output size is the same size as the input size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxyWI-KQ123C"
      },
      "source": [
        "### Adding a Pooling Layer \n",
        "* To **reduce overfitting** and **computation time**, a **convolution layer** is often followed by one or more layers that **reduce dimensionality** of **convolution layer’s output**\n",
        "* **Pooling compresses** (or **down-samples**) the results by **discarding features**\n",
        "    * Helps make the model **more general**\n",
        "* **Most common pooling technique** is called **max pooling**\n",
        "\t* Examines a 2-by-2 square of features and keeps only the maximum feature.\n",
        "\n",
        "### Adding a Pooling Layer (cont.)\n",
        "* 2-by-2 blue square in position 1 represents the initial pool of features to examine:\n",
        "\n",
        "![Max pooling diagram showing the 6-by-6 set of numeric values we wish to compress with the 2-by-2 blue square in position 1 representing the initial pool of features to examine, and the 3-by-3 square representing the results of max pooling](images/pooling.png \"Max pooling diagram showing the 6-by-6 set of numeric values we wish to compress with the 2-by-2 blue square in position 1 representing the initial pool of features to examine, and the 3-by-3 square representing the results of max pooling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6venyXNNHbl"
      },
      "source": [
        "# Classification \n",
        "\n",
        "The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.\n",
        "\n",
        "After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.\n",
        "\n",
        "Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n",
        "\n",
        "Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.\n",
        "\n",
        "![](images/A_CNN_sequence_to_classify_handwritten_digits.jpg)"
      ]
    }
  ]
}