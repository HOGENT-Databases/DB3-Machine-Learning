{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree Classification\n",
    "This algorithm is a widely-used method of constructing a model from a dataset in the form of a decision tree. It is often claimed that this representation of the data has the advantage **compared with other approaches** of being **meaningful** and **easy to interpret**.  \n",
    "  \n",
    "As an example you can consider a database in use at a real-estate company. This database contains customer data (including age) and rental history. The company wants to predict how likely it is that a renter will buy a house, based on historical data about buyers and renters.  \n",
    "  \n",
    "A tree classification algorithm will create a tree classifier as in the figure below. \n",
    "  \n",
    "![](images/tree.png)\n",
    "  \n",
    "We can conclude from this tree classification that only customers that are renting for over two years and that are older than 25 years are considering to buy a property. The real-estate company can use this information to advertise properties for sale to this customer segment.  \n",
    "  \n",
    "The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes. The trick, of course, comes in deciding which questions to ask at each step.   \n",
    "  \n",
    "In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data; that is, each node in the tree splits the data into two groups using a cutoff value within one of the features. Let’s now take a look at an example.  \n",
    "### Creating a decision tree\n",
    "Consider the following two-dimensional data, which has one of four class labels (VanderPlas, 2016).  \n",
    "  \n",
    "![](images/treedots.png)\n",
    "  \n",
    "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it. The next image presents a visualization of the first four levels of a decision tree classifier for this data.  \n",
    "  \n",
    "![](images/treesplits.png)\n",
    "\n",
    "Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch. Except for nodes that contain all of one color, at each level every region is again split along one of the two features.  \n",
    "  \n",
    "Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of four, there is a tall and skinny purple region between the yellow and blue regions. It’s clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data. That is, this decision tree, even at only five levels deep, is clearly **overfitting** our data.\n",
    "### Decision trees and overfitting\n",
    "Such overfitting turns out to be a general property of decision trees; it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from. \n",
    "### Ensembles of Estimators: Random Forests\n",
    "We can also see that if we train models on different subsets of the data – for example, we train two different trees, each on half of the original data, the two trees produce consistent results for some combinations of the features, while in other places, the two trees give very different results. The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from both of these trees, we might come up with a better result!  This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an _ensemble method_ called _bagging_. Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which overfits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a **random forest**.  \n",
    "  \n",
    "Because random forests train a set of decision trees separately, the training can be done in parallel. The algorithm injects randomness into the training process so that each decision tree is a bit different. Combining the predictions from each tree reduces the variance of the predictions, improving the performance on test data.  \n",
    "  \n",
    "The **randomness** injected into the training process includes:\n",
    "- Subsampling the original dataset on each iteration to get a different training set (a.k.a. bootstrapping).\n",
    "- Considering different random subsets of features to split on at each tree node.  \n",
    "  \n",
    "Apart from these randomizations, decision tree training is done in the same way as for individual decision trees.\n",
    "To make a prediction on a new instance, a random forest must **aggregate the predictions from its set of decision trees**. This aggregation is done differently for classification and regression.  \n",
    "  \n",
    "- _Classification_ : Majority vote. Each tree’s prediction is counted as a vote for one class. The label is predicted to be the class which receives the most votes.\n",
    "- _Regression_ : Averaging. Each tree predicts a real value. The label is predicted to be the average of the tree predictions.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
